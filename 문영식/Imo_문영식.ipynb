{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSFahmSoedJWENLPlQ+n7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungsikMoon/Find_Imo/blob/main/%EB%AC%B8%EC%98%81%EC%8B%9D/Imo_%EB%AC%B8%EC%98%81%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT 예시 코드"
      ],
      "metadata": {
        "id": "EJoJDPDxQ67T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 임포트"
      ],
      "metadata": {
        "id": "jAu00hw-XD_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n"
      ],
      "metadata": {
        "id": "bM3GraWOPFvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT 모델 구현"
      ],
      "metadata": {
        "id": "mgvvN7JAXHzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = image_size\n",
        "        patch_height, patch_width = patch_size\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "1O0SBetQXKLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 트랜스 포머 정의"
      ],
      "metadata": {
        "id": "VTKHtrUVW-BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "\n",
        "        dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n"
      ],
      "metadata": {
        "id": "vHWf6gpRXABz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT로 분류 구현해보기"
      ],
      "metadata": {
        "id": "S6md7dWSRBAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 패키지 인스톨"
      ],
      "metadata": {
        "id": "VbzPnxrQXQPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm # 꼭 설치 하세요. 그리고 세션 재시작"
      ],
      "metadata": {
        "id": "U46R2G4mPB65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리 임포트"
      ],
      "metadata": {
        "id": "7VKEzfHZXUcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리들을 임포트합니다.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet50\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torchvision.models import resnet50\n",
        "from timm.models import vit_base_patch16_224"
      ],
      "metadata": {
        "id": "hmIu6L1XXbBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 분류 코드 구현"
      ],
      "metadata": {
        "id": "lDMGwGyhXbgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리를 위한 변환(Transform)을 정의합니다.\n",
        "# 여기서는 이미지의 크기를 조정하고, 텐서로 변환하며, 정규화를 수행합니다.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 이미지의 크기를 224x224로 조정합니다.\n",
        "    transforms.ToTensor(), # 이미지를 PyTorch 텐서로 변환합니다.\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 이미지를 정규화합니다.\n",
        "])\n",
        "\n",
        "# ImageNet 데이터셋을 로드합니다.\n",
        "# 여기서는 학습 데이터셋과 검증 데이터셋을 각각 로드합니다.\n",
        "train_dataset = datasets.ImageFolder(root='./data/ImageNet/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='./data/ImageNet/val', transform=transform)\n",
        "\n",
        "# 데이터 로더를 생성합니다.\n",
        "# 데이터 로더는 학습 중에 배치 단위로 데이터를 제공합니다.\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Vision Transformer 모델을 정의합니다.\n",
        "# 여기서는 timm 라이브러리에서 제공하는 vit_base_patch16_224 모델을 사용하며,\n",
        "# 이 모델은 이미 ImageNet 데이터셋으로 사전 학습된 상태입니다.\n",
        "model = vit_base_patch16_224(pretrained=True)\n",
        "model.head = nn.Linear(model.head.in_features, 1000) # ImageNet은 1000개의 클래스를 가집니다.\n",
        "\n",
        "# GPU를 사용할 수 있는지 확인하고, 사용 가능하면 모델을 GPU로 이동시킵니다.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# 손실 함수와 옵티마이저를 정의합니다.\n",
        "# 여기서는 크로스 엔트로피 손실 함수와 Adam 옵티마이저를 사용합니다.\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 함수를 정의합니다.\n",
        "# 이 함수는 주어진 에폭 수만큼 모델을 학습합니다.\n",
        "def train(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train() # 모델을 학습 모드로 설정합니다.\n",
        "        for i, (images, labels) in enumerate(train_loader): # 학습 데이터셋에서 배치 단위로 데이터를 가져옵니다.\n",
        "            images = images.to(device) # 이미지 데이터를 GPU로 이동시킵니다.\n",
        "            labels = labels.to(device) # 레이블 데이터를 GPU로 이동시킵니다.\n",
        "\n",
        "            outputs = model(images) # 모델을 통해 이미지 데이터를 전달하고 출력을 얻습니다.\n",
        "            loss = criterion(outputs, labels) # 손실 함수를 사용하여 손실을 계산합니다.\n",
        "\n",
        "            optimizer.zero_grad() # 옵티마이저의 그래디언트를 초기화합니다.\n",
        "            loss.backward() # 손실에 대해 역전파를 수행하여 그래디언트를 계산합니다.\n",
        "            optimizer.step() # 옵티마이저를 사용하여 모델의 파라미터를 업데이트합니다.\n",
        "\n",
        "        model.eval() # 모델을 평가 모드로 설정합니다.\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad(): # 그래디언트 계산을 비활성화합니다.\n",
        "            for images, labels in test_loader: # 검증 데이터셋에서 배치 단위로 데이터를 가져옵니다.\n",
        "                images = images.to(device) # 이미지 데이터를 GPU로 이동시킵니다.\n",
        "                labels = labels.to(device) # 레이블 데이터를 GPU로 이동시킵니다.\n",
        "\n",
        "                outputs = model(images) # 모델을 통해 이미지 데이터를 전달하고 출력을 얻습니다.\n",
        "                _, predicted = torch.max(outputs.data, 1) # 가장 높은 확률을 가진 클래스를 예측값으로 선택합니다.\n",
        "                total += labels.size(0) # 전체 샘플 수를 계산합니다.\n",
        "                correct += (predicted == labels).sum().item() # 정확히 예측한 샘플 수를 계산합니다.\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {100 * correct / total:.2f}%') # 학습 진행 상황을 출력합니다.\n",
        "\n",
        "# 모델 학습을 시작합니다.\n",
        "train(model, criterion, optimizer, train_loader, test_loader, epochs=10)\n"
      ],
      "metadata": {
        "id": "Ghe1jAbPRL_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI 면접 서비스 구현"
      ],
      "metadata": {
        "id": "uad9h_59eHva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "요구 기능 기본\n",
        "- 이미지 파일 업로드.\n",
        "- 분류 값 출력.\n",
        "\n",
        "요구 기능 심화\n",
        "- 카메라 온\n",
        "- TTS로 질문\n",
        "- STT로 답변\n",
        "- 답변 시 카메라 캡쳐\n",
        "- 데이터프레임 구성 [질문, 답변, 캡쳐이미지, 평가값]"
      ],
      "metadata": {
        "id": "3OeA6JTneNAx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgN5R7eqe_k2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}